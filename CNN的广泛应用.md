## 神经网络主要结构
    神经网络主要目的是对给定对一组数据，找到其映射关系函数f，得到f(x)=wx+b中常量w，b的值,计算预测值。
    基础单元为神经元（感知机），每个神经元都与下一层相接所做的事是线性变换或非线性变换，输入向量，输出标量。
    从结构上可分为：输入层，隐藏层，输出层，比如一个神经网络有3层，一般包括输入层1，隐藏层2，输出层1。
    从参数结构类型可分为：DNN（深度神经网络），CNN（卷积神经网络），RNN（循环神经网络），每一种网络都有其对应的参数，常用激活函数。



## 常见激活函数有几种
    深度神经网络激活函数包括以下几种：
    Sigmod函数：特征相差不大的情况下适合使用该方法，将一个实数映射到(0,1)区间，但是在分布比较平滑的地方，梯度容易消失。
    Tanh函数：取值范围(-1,1)，tanh(x)=sinh(x）/ cosh(x），tanh函数有比sigmoid更快的收敛速度，从而降低来迭代所需次数。
    ReLU函数：大于0的输入数据可正常计算，但是小于0的部分可能被省略，造成更大误差。
    softmax函数：多分类问题每一个可能性都计算出来，它将多个神经元的输出映射到(0,1)空间，用于多分类问题，每一个分类的可能性都可以计算出来。
    
## DNN 算法主要步骤？
    正向传播: 得到参数确认参数的过程 常量w，b的值,计算预测值
    反向传播: 求偏导，求的损失函数
    梯度下降: 更新参数，迭代更新参数，得到最小损失函数，接近真实值的为最优解。
    
    
## 深度学习主要应用场景
    深度学习具有优秀的自动提取特征的能力，能够学习多层次的抽象特征表示，并对异质或跨域的内容信息进行学习
    图像识别
        基于深度学习的图像分类方法，通过有监督无监督的方法学习层次化的特征描述，从而取得手工设计或选择图像特征的工作，比如效果较好的CNN直接使用
        图像像素信息作为输入，最大程度保留图像的信息，通过卷积操作进行特征提取和高层操作，模型输出直接是图像识别结果。这就是输入-输出，端到端的学习方法效果很好。
        基础任务
            图形分类
            物体跟踪，行为分析，目标检测，图像分割
        数字识别
        文字识别
   
    自然语言处理
        应用: 搜索引擎，广告系统，推荐系统
        词向量:
            我们经常要比较两个词或者两段文本之间的相关性。
            向量空间模型(vector space model)。 
                在这种模型里，每个词被表示成一个实数向量（one-hot vector），其长度为字典大小，每个维度对应一个字典里的每个词，除了这个词对应维度上的值是1，其他元素都是0。
            每个词本身的信息量很小。所以，仅仅给定两个词，不足以让我们准确判别它们是否相关。
            要想精确计算相关性，我们还需要更多的信息——从大量数据里通过机器学习方法归纳出来的知识
        
        词向量模型:
            词向量模型可以是概率模型、共生矩阵(co-occurrence matrix)模型或神经元网络模型。在用神经网络求词向量之前，传统做法是统计一个词语的共生矩阵X。
            X是一个|V|×|V| 大小的矩阵，Xij表示在所有语料中，词汇表V(vocabulary)中第i个词和第j个词同时出现的词数，|V|为词汇表的大小。
            对X做矩阵分解（如奇异值分解，Singular Value Decomposition [5]）
            但这样的传统做法有很多问题：

            由于很多词没有出现，导致矩阵极其稀疏，因此需要对词频做额外处理来达到好的矩阵分解效果；
            矩阵非常大，维度太高(通常达到106×106的数量级)；
            需要手动去掉停用词（如although, a,...），不然这些频繁出现的词也会影响矩阵分解的效果。
            基于神经网络的模型不需要计算和存储一个在全语料上统计产生的大表，而是通过学习语义信息得到词向量，因此能很好地解决以上问题
        基于神经网络的词向量模型
            语言模型旨在为语句的联合概率函数P(w1,...,wT)建模, 其中wi表示句子中的第i个词。语言模型的目标是，希望模型对有意义的句子赋予大概率，对没意义的句子赋予小概率。 
            这样的模型可以应用于很多领域，如机器翻译、语音识别、信息检索、词性标注、手写识别等，它们都希望能得到一个连续序列的概率。 
            以信息检索为例，当你在搜索“how long is a football bame”时（bame是一个医学名词），搜索引擎会提示你是否希望搜索"how long is a football game", 
            这是因为根据语言模型计算出“how long is a football bame”的概率很低，而与bame近似的，可能引起错误的词中，game会使该句生成的概率最大。

            对语言模型的目标概率P(w1,...,wT)，如果假设文本中每个词都是相互独立的，则整句话的联合概率可以表示为其中所有词语条件概率的乘积
            N-gram neural model
                n-gram 是文本语言模型的重要表示方法
                n-gram模型也是统计语言模型中的一种重要方法，用n-gram训练语言模型时，一般用每个n-gram的历史n-1个词语组成的内容来预测第n个词。
                神经概率语言模型（Neural Network Language Model，NNLM）通过一个线性映射和一个非线性隐层连接，同时学习了语言模型和词向量，即通过学习大量语料得到词语的向量表达，通过这些向量得到整个句子的概率
                
                
            Continuous Bag-of-Words model(CBOW)
                CBOW模型通过一个词的上下文（各N个词）预测当前词。 
                
            Skip-gram model
                CBOW的好处是对上下文词语的分布在词向量上进行了平滑，去掉了噪声，因此在小数据集上很有效。而Skip-gram的方法中，用一个词预测其上下文，
                得到了当前词上下文的很多样本，因此可用于更大的数据集
                
    个性化推荐(Recommender System)
        信息超载: 技术不断发展和电子商务数量和种类快速增长，用户需要大量时间才能找到自己想要的商品
        个性化推荐系统可以解决以上问题，不需要用户准确地描述出自己的需求，而是根据用户的历史行为进行建模，主动提供满足用户兴趣和需求的信息
        主要方法:
            协同过滤推荐（Collaborative Filtering Recommendation）：该方法是应用最广泛的技术之一，需要收集和分析用户的历史行为、活动和偏好
                它不依赖于机器去分析物品的内容特征，因此它无需理解物品本身也能够准确地推荐诸如电影之类的复杂物品；缺点是对于没有任何行为的新用户存在冷启动的问题，同时也存在用户与商品之间的交互数据不够多造成的稀疏问题。值得一提的是，社交网络[3]或地理位置等上下文信息都可以结合到协同过滤中去。
            基于内容过滤推荐
                该方法利用商品的内容描述，抽象出有意义的特征，通过计算用户的兴趣和商品描述之间的相似度，来给用户做推荐。优点是简单直接，不需要依据其他用户对商品的评价，而是通过商品属性进行商品相似度度量，从而推荐给用户所感兴趣商品的相似商品；缺点是对于没有任何行为的新用户同样存在冷启动的问题
            组合推荐
                运用不同的输入和技术共同进行推荐，以弥补各自推荐技术的缺点
                
                
    情感分析
        情感分析一般是指判断一段文本所表达的情绪状态。其中，一段文本可以是一个句子，一个段落或一个文档。情绪状态可以是两类，
        如（正面，负面），（高兴，悲伤）；也可以是三类，如（积极，消极，中性）等等。
        经典文本表示方法:
            词袋模型BOW(bag of words)
                在一段文本，BOW表示会忽略其词顺序、语法和句法，将这段文本仅仅看做是一个词集合，因此BOW方法并不能充分表示文本的语义信息。
            话题模型
        分类方法
            支持向量机 SVM(support vector machine)
            逻辑回归 LR(logistic regression)
        
        
        神经网络
            使用卷积处理输入的词向量序列，产生一个特征图(feature map)，
            对特征图采用时间维度的最大池化(max pooling over time)得到此卷积对应的整句话的特征
            最后将所有卷积核得到的特征拼接起来即为文本的定长向量表示，对于文本分类问题，将其连接至softmax即构建出完整的模型
            
        循环神经网络RNN
            循环神经网络（RNN）
            循环神经网络是一种能对序列数据进行精确建模的有力工具。实际上，循环神经网络的理论计算能力是图灵完备的[4]。
            自然语言是一种典型的序列数据（词序列），近年来，循环神经网络及其变体（如long short term memory[5]等）在自然语言处理的多个领域，
            如语言模型、句法解析、语义角色标注（或一般的序列标注）、语义表示、图文生成、对话、机器翻译等任务上均表现优异甚至成为目前效果最好的方法
        
        长短期记忆网络LSTM(long short term memory)
            为解决循环神经网络训练过程中容易出现的梯度消失或梯度爆炸的问题而提出
            LSTM增加了记忆单元c、输入门i、遗忘门f及输出门o。这些门及记忆单元组合起来大大提升了循环神经网络处理长序列数据的能力。若将基于LSTM的循环神经网络表示的函数记为 F
        
        栈式双向LSTM（Stacked Bidirectional LSTM）
            正常顺序的循环神经网络，ht包含了t时刻之前的输入信息，也就是上文信息。同样，为了得到下文信息，我们可以使用反方向（将输入逆序处理）的循环神经网络。结合构建深层循环神经网络的方法（深层神经网络往往能得到更抽象和高级的特征表示），我们可以通过构建更加强有力的基于LSTM的栈式双向循环神经网络[9]，来对时序数据进行建模
            
    语义角色标注
        词法分析、句法分析和语义分析是NLP的三大层面
        完全句法分析需要确定句子所包含的全部句法信息，并确定句子各成分之间的关系，是一个非常困难的任务，目前技术下的句法分析准确率并不高，句法分析的细微错误都会导致SRL的错误
        语义角色是指论动词所指事件中担任的角色。
        主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等
        语义角色标注（Semantic Role Labeling，SRL）以句子的谓词为中心，不对句子所包含的语义信息进行深入分析，只分析句子中各成分与谓词之间的关系，
        即句子的谓词（Predicate）- 论元（Argument）结构，并用语义角色来描述这些结构关系，是许多自然语言理解任务（如信息抽取，篇章分析，深度问答等）的一个重要中间步骤。在研究中一般都假定谓词是给定的，所要做的就是找出给定谓词的各个论元和它们的语义角色。
        传统的SRL系统大多建立在句法分析基础之上，通常包括5个流程：

        构建一棵句法分析树
        从句法树上识别出给定谓词的候选论元。
        候选论元剪除；一个句子中的候选论元可能很多，候选论元剪除就是从大量的候选项中剪除那些最不可能成为论元的候选项。
        论元识别：这个过程是从上一步剪除之后的候选中判断哪些是真正的论元，通常当做一个二分类问题来解决。
        对第4步的结果，通过多分类得到论元的语义角色标签。可以看到，句法分析是基础，并且后续步骤常常会构造的一些人工特征，这些特征往往也来自句法分析
        
        模型:
            栈式循环神经网络（Stacked Recurrent Neural Network
            双向循环神经网络（Bidirectional Recurrent Neural Network）
            条件随机场 (Conditional Random Field)
            深度双向LSTM（DB-LSTM）SRL模型
    
    机器翻译
        GRU
        循环神经网络（RNN）及长短时间记忆网络（LSTM）。相比于简单的RNN，LSTM增加了记忆单元（memory cell）、输入门（input gate）、遗忘门（forget gate）及输出门（output gate）
        这些门及记忆单元组合起来大大提升了RNN处理远距离依赖问题的能力。

        GRU[2]是Cho等人在LSTM上提出的简化版本，也是RNN的一种扩展，如下图所示。GRU单元只有两个门：

        重置门（reset gate）：如果重置门关闭，会忽略掉历史信息，即历史不相干的信息不会影响未来的输出。
        更新门（update gate）：将LSTM的输入门和遗忘门合并，用于控制历史信息对当前时刻隐层输出的影响。如果更新门接近1，会把历史信息传递下去。
        
        双向循环神经网络
        
        编码器-解码器框架
        
        注意力机制
        
        柱搜索算法
        
## 生成对抗网络（Generative Adversarial Network [1]，简称GAN）
     非监督式学习的一种方法，通过让两个神经网络相互博弈的方式进行学习
     生成对抗网络由一个生成网络与一个判别网络组成。生成网络从潜在空间（latent space）中随机采样作为输入，其输出结果需要尽量模仿训练集中的真实样本。
     判别网络的输入为真实样本或生成网络的输出，其目的是将生成网络的输出从真实样本中尽可能分辨出来。而生成网络则要尽可能地欺骗判别网络。
     两个网络相互对抗、不断调整参数，其目的是将生成网络生成的样本和真实样本尽可能的区分
     
     GAN 网络顾名思义，是一种通过对抗的方式，去学习数据分布的生成模型。其中，“对抗”指的是生成网络（Generator)和判别网络（Discriminator)的相互对抗。这里以生成图片为例进行说明：

     生成网络（G）接收一个随机的噪声z，尽可能的生成近似样本的图像，记为G(z)
     判别网络（D）接收一张输入图片x，尽可以去判别该图像是真实样本还是网络生成的假样本，判别网络的输出 D(x) 代表 x 为真实图片的概率。
     如果 D(x)=1 说明判别网络认为该输入一定是真实图片，如果 D(x)=0 说明判别网络认为该输入一定是假图片。

     应用:
        生成以假乱真的图片。
        生成视频
        三维物体模型
     
     模型:
        GAN
        DCGAN
            是深层卷积网络与 GAN 的结合，其基本原理与 GAN 相同，只是将生成网络和判别网络用两个卷积网络（CNN）替代。为了提高生成样本的质量和网络的收敛速度